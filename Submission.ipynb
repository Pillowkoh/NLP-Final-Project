{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_file(filename):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # Split the entire file into sentences. Output: List of sentences\n",
    "    sentences = file_content.strip().split('\\n\\n')\n",
    "\n",
    "    # Split each sentence into their token_tag pair\n",
    "    # Output: List of sentences. Each sentence is a list of token_tag_pair\n",
    "    token_tag_pairs = [i.split('\\n') for i in sentences]\n",
    "\n",
    "    # Separate each token_tag_pair into a list of [token, tag].\n",
    "    # Output: [[[token, tag], [token, tag], ...], [[token, tag], [token, tag], ...], ...]\n",
    "    for idx, sentence in enumerate(token_tag_pairs):\n",
    "        token_tags = [i.rsplit(' ', maxsplit=1) for i in sentence]\n",
    "        token_tag_pairs[idx] = token_tags\n",
    "\n",
    "    return token_tag_pairs\n",
    "\n",
    "def read_dev_in_file(filename):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # Split the entire file into sentences. Output: List of sentences\n",
    "    sentences = file_co\n",
    "    ntent.strip().split('\\n\\n')\n",
    "\n",
    "    # Split each sentence into their tokens\n",
    "    # Output: List of sentences. Each sentence is a list of tokens\n",
    "    tokens = [i.split('\\n') for i in sentences]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './dataset/train'\n",
    "dev_dataset_path = './dataset/dev.in'\n",
    "\n",
    "train_dataset = read_train_file(train_dataset_path)\n",
    "dev_dataset = read_dev_in_file(dev_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_states(train_dataset):\n",
    "    possible_states = []\n",
    "\n",
    "    for sentence in train_dataset:\n",
    "        for token, tag in sentence:\n",
    "            if tag not in possible_states:\n",
    "                possible_states.append(tag)\n",
    "\n",
    "    return possible_states\n",
    "\n",
    "possible_states = get_possible_states(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_counts(train_dataset):\n",
    "    emission_count = {}\n",
    "    transition_count = {}\n",
    "    state_count = {}\n",
    "    \n",
    "\n",
    "    transition_count['START'] = {}\n",
    "    for sentence in train_dataset:\n",
    "        prev_state = None\n",
    "        \n",
    "        for token, tag in sentence:\n",
    "\n",
    "            if emission_count.get(token) == None:\n",
    "                emission_count[token] = {}\n",
    "            emission_count[token][tag] = emission_count[token].get(tag, 0) + 1\n",
    "\n",
    "            if prev_state != None:\n",
    "                if transition_count.get(prev_state) == None:\n",
    "                    transition_count[prev_state] = {}\n",
    "                transition_count[prev_state][tag] = transition_count[prev_state].get(tag, 0) + 1\n",
    "\n",
    "            else:\n",
    "                transition_count['START'][tag] = transition_count['START'].get(tag, 0) + 1\n",
    "                state_count['START'] = state_count.get('START', 0) + 1\n",
    "\n",
    "            state_count[tag] = state_count.get(tag, 0) + 1\n",
    "            prev_state = tag\n",
    "\n",
    "        transition_count[prev_state]['STOP'] = transition_count[prev_state].get('STOP', 0) + 1\n",
    "\n",
    "    return emission_count, transition_count, state_count\n",
    "\n",
    "e_count, t_count, state_count = get_feature_counts(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dictionary(e_count, t_count, state_count):\n",
    "    f = {}\n",
    "    for token, tags in e_count.items():\n",
    "        for tag, e_count in tags.items():\n",
    "            key = \"emission: \" + tag + '+' + token\n",
    "            e_prob = np.log(e_count/state_count[tag])\n",
    "            f[key] = e_prob\n",
    "\n",
    "    for prev_tag, next_tags in t_count.items():\n",
    "        for next_tag, t_count in next_tags.items():\n",
    "            key = \"transition: \" + prev_tag + '+' + next_tag\n",
    "            t_prob = np.log(t_count/state_count[prev_tag])\n",
    "            f[key] = t_prob\n",
    "\n",
    "    return f\n",
    "\n",
    "f = get_feature_dictionary(e_count, t_count, state_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "calculate_score(x,y):\n",
    "Helps to calulate the score for a given pair of input and output sequence pair (x,y)\n",
    "Based on 2 features, emission and transition\n",
    "\n",
    "Parameters:\n",
    "x: List of tokens, e.g. x = x1, x2, ..., xn         Type: list[str]\n",
    "y: List of tags, e.g. y = y1, y2, ..., yn           Type: list[str]\n",
    "f: Dictionary of feature weights                    Type: Dict{features: weights}\n",
    "'''\n",
    "\n",
    "def calculate_score(x,y,f):\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "    feature_count = {}\n",
    "\n",
    "    prev_tag = 'START'\n",
    "    score = 0\n",
    "\n",
    "    length = len(x)\n",
    "    for i in range(length):\n",
    "        e_key = \"emission: \" + y[i] + '+' + x[i]\n",
    "        t_key = \"transition: \" + prev_tag + '+' + y[i]\n",
    "\n",
    "        if e_key in f.keys():\n",
    "            feature_count[e_key] = feature_count.get(e_key, 0) + 1\n",
    "\n",
    "        if t_key in f.keys():\n",
    "            feature_count[t_key] = feature_count.get(t_key, 0) + 1\n",
    "\n",
    "        prev_tag = y[i]\n",
    "        \n",
    "    t_key = \"transition: \" + prev_tag + '+' + 'STOP'\n",
    "    if t_key in f.keys():\n",
    "        feature_count[t_key] = feature_count.get(t_key, 0) + 1\n",
    "\n",
    "    for feature, count in feature_count.items():\n",
    "        score += f[feature] * count\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(x, possible_states, f, default_index=0):\n",
    "    n = len(x)\n",
    "    d = len(possible_states)\n",
    "    scores = np.full((n, d), -np.inf)\n",
    "    bp = np.full((n, d), default_index, dtype=np.int32)\n",
    "\n",
    "    for i in range(len(possible_states)):\n",
    "        t_key = \"transition: START\"+possible_states[i]\n",
    "        e_key = \"emission: \"+possible_states[i]+\"+\"+x[0]\n",
    "        t_prob = f.get(t_key, -2**31)\n",
    "        e_prob = f.get(e_key, -2**31)\n",
    "        scores[0, i] = t_prob + e_prob\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        for k in range(len(possible_states)):\n",
    "            for j in range(len(possible_states)):\n",
    "                t_key = \"transition: \"+possible_states[k]+\"+\"+possible_states[j]\n",
    "                e_key = \"emission: \"+possible_states[j]+\"+\"+x[i]\n",
    "                t_prob = f.get(t_key, -2**31)\n",
    "                e_prob = f.get(e_key, -2**31)\n",
    "                overall_score = e_prob + t_prob + scores[i-1, k]\n",
    "                if overall_score > scores[i, j]:\n",
    "                    scores[i, j] = overall_score\n",
    "                    bp[i,j] = k\n",
    "    \n",
    "    highest_score = -2**31\n",
    "    highest_bp = default_index\n",
    "    for i in range(len(possible_states)):\n",
    "        t_key = \"transition: \"+possible_states[i]+\"+STOP\"\n",
    "        t_prob = f.get(t_key, -2**31)\n",
    "        overall_score = t_prob + scores[n-1, i]\n",
    "        \n",
    "        if overall_score > highest_score:\n",
    "            highest_score = overall_score\n",
    "            highest_bp = i\n",
    "    \n",
    "    result = [possible_states[highest_bp]]\n",
    "    prev_bp = highest_bp\n",
    "    for i in range(n-1, 0, -1):\n",
    "        prev_bp = bp[i, prev_bp]\n",
    "        output = possible_states[prev_bp]\n",
    "        result = [output] + result\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(path, states, f, output_filename):\n",
    "    default_index = states.index('O')\n",
    "    sentences = list()\n",
    "\n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "        sentence = list()\n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()   \n",
    "            \n",
    "            if(len(formatted_line) ==0):\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            sentence.append(formatted_line)\n",
    "\n",
    "    with open(output_filename, \"w\") as wf:\n",
    "        for sentence in sentences:\n",
    "            pred_sentence = viterbi(sentence, states, f, default_index)        \n",
    "            for i in range(len(sentence)):\n",
    "                wf.write(sentence[i] + \" \" + pred_sentence[i] + \"\\n\")\n",
    "                \n",
    "            wf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(\"dataset/dev.in\", possible_states, f, 'dataset/dev.p2.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 3809 tokens with 210 phrases; found: 168 phrases; correct: 66.\n",
      "accuracy:  28.62%; (non-O)\n",
      "accuracy:  92.57%; precision:  39.29%; recall:  31.43%; FB1:  34.92\n",
      "         negative: precision:  32.26%; recall:  15.38%; FB1:  20.83  31\n",
      "          neutral: precision:   7.14%; recall:  12.50%; FB1:   9.09  14\n",
      "         positive: precision:  44.72%; recall:  40.15%; FB1:  42.31  123\n",
      "(39.285714285714285, 31.428571428571427, 34.920634920634924)\n"
     ]
    }
   ],
   "source": [
    "from conlleval import evaluate, evaluate_conll_file\n",
    "\n",
    "def get_tags(pred,gold):\n",
    "    f_pred = open(pred,encoding = 'utf-8')\n",
    "    f_gold = open(gold,encoding = 'utf-8')\n",
    "    data_pred = f_pred.readlines()\n",
    "    data_gold = f_gold.readlines()\n",
    "    gold_tags = list()\n",
    "    pred_tags = list()\n",
    "    \n",
    "    for sentence in range(len(data_gold)):\n",
    "        words_pred = data_pred[sentence].strip().split(' ')\n",
    "        words_gold = data_gold[sentence].strip().split(' ')  \n",
    "        if len(words_gold)==1:\n",
    "            continue\n",
    "        # Write original word and predicted tags\n",
    "        gold_tags.append(words_gold[1])\n",
    "        pred_tags.append(words_pred[1])\n",
    "        # End of sentence, write newline\n",
    "    return gold_tags,pred_tags\n",
    "\n",
    "\n",
    "g_tags, p_tags = get_tags('dataset/dev.p2.out', 'dataset/dev.out')\n",
    "print(evaluate(g_tags,p_tags,verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_algorithm(x, f, possible_states):\n",
    "    forward_scores = np.zeros((len(x), len(possible_states)))\n",
    "    threshold = 700\n",
    "\n",
    "    for i in range(len(possible_states)):\n",
    "        t_key = \"transition: START+\"+possible_states[i]\n",
    "        t_prob = f.get(t_key, -2**31)\n",
    "        e_key = \"emission: \"+possible_states[i]+\"+\"+x[0]\n",
    "        e_prob = f.get(e_key, -2**31)\n",
    "        forward_scores[0, i] = t_prob + e_prob\n",
    "    \n",
    "    for i in range(1, len(x)):\n",
    "        for k in range(len(possible_states)):\n",
    "            temp_score = 0\n",
    "            for j in range(len(possible_states)):\n",
    "                t_key = \"transition: \"+possible_states[j]+\"+\"+possible_states[k]\n",
    "                t_prob = f.get(t_key, -2**31)\n",
    "                e_key = \"emission: \"+possible_states[k]+\"+\"+x[i]\n",
    "                e_prob = f.get(e_key, -2**31)\n",
    "                score = e_prob + t_prob + forward_scores[i-1,j]\n",
    "                if score > threshold:\n",
    "                    score = threshold\n",
    "                temp_score += np.exp(score)\n",
    "                \n",
    "            forward_scores[i, k] = np.log(temp_score) if temp_score else -2**31\n",
    "\n",
    "    forward_prob = 0\n",
    "    for j in range(len(possible_states)):\n",
    "        t_key = \"transition: \"+possible_states[j]+\"+STOP\"\n",
    "        t_prob = f.get(t_key, -2**31)\n",
    "        score = t_prob + forward_scores[len(x)-1,j]\n",
    "        if score > threshold:\n",
    "            score = threshold\n",
    "        overall_score = np.exp(score)\n",
    "        forward_prob += overall_score\n",
    "    if forward_prob > 0:\n",
    "        alpha = np.log(forward_prob)\n",
    "    else:\n",
    "        alpha = threshold\n",
    "        \n",
    "    return forward_scores, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRF_loss(train_dataset,f,possible_states):\n",
    "    temp_loss = 0\n",
    "    for sentence in train_dataset:\n",
    "        x = [token_tag_pair[0] for token_tag_pair in sentence]\n",
    "        y = [token_tag_pair[1] for token_tag_pair in sentence]\n",
    "        _, alpha = forward_algorithm(x, f, possible_states)\n",
    "        temp_loss += calculate_score(x,y,f) - alpha\n",
    "\n",
    "    crf_loss = -temp_loss\n",
    "    return crf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2050.7405338353615"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRF_loss(train_dataset,f,possible_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 (ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_algorithm(x, f, possible_states):\n",
    "    \n",
    "    backward_scores = np.zeros((len(x), len(possible_states)))\n",
    "    threshold = 700\n",
    "    \n",
    "    for i in range(len(possible_states)):\n",
    "        t_key = \"transition: \"+ possible_states[i] +\"+STOP\"\n",
    "        t_prob = f.get(t_key, -2**31)\n",
    "        backward_scores[len(x)-1, i] = t_prob\n",
    "\n",
    "    for i in range(len(x)-1,0,-1):\n",
    "        for j  in range(len(possible_states)):\n",
    "            temp_score = 0 \n",
    "            for k in range(len(possible_states)):\n",
    "                t_key = \"transition: \" + possible_states[j] + \"+\" + possible_states[k]\n",
    "                e_key = \"emission: \"+ possible_states[k] + \"+\" + x[i] \n",
    "                t_prob = f.get(t_key, -2**31)\n",
    "                e_prob = f.get(e_key, -2**31)\n",
    "                temp_score += np.exp(min(e_prob + t_prob + backward_scores[i, k], threshold))\n",
    "            \n",
    "            if temp_score!=0:\n",
    "                backward_scores[i-1, j] = np.log(temp_score)\n",
    "            else:\n",
    "                backward_scores[i-1, j] = -2**31\n",
    "    \n",
    "    backward_prob = 0\n",
    "    for i in range(len(possible_states)):\n",
    "        t_key = \"transition: \" + \"START\" + \"+\" + possible_states[i]\n",
    "        e_key = \"emission: \" + possible_states[i] + \"+\" + x[0]\n",
    "        t_prob = f.get(t_key, -2**31)\n",
    "        e_prob = f.get(e_key, -2**31)\n",
    "        overall_score = np.exp(min(e_prob + t_prob + backward_scores[0, i], threshold))\n",
    "        backward_prob += overall_score\n",
    "        \n",
    "    if backward_prob!=0:\n",
    "        beta = np.log(backward_prob)\n",
    "    else:\n",
    "        beta = -threshold    \n",
    "\n",
    "    return backward_scores, beta\n",
    "\n",
    "\n",
    "def forward_backward(x, f, possible_states):\n",
    "    \n",
    "    threshold = 700\n",
    "    \n",
    "    forward_scores, alpha = forward_algorithm(x, f, possible_states)\n",
    "    forward_prob = np.exp(min(alpha, threshold))\n",
    "    backward_scores, beta = backward_algorithm(x, f, possible_states)\n",
    "    backward_prob = np.exp(min(beta, threshold))\n",
    "    feature_expected_counts = {}\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(possible_states)):\n",
    "            e_key = \"emission: \" + possible_states[j] + \"+\" + x[i]\n",
    "            feature_expected_counts[e_key] = feature_expected_counts.get(e_key, 0.0) + np.exp(min(forward_scores[i, j] + backward_scores[i, j] - alpha, threshold))\n",
    "    \n",
    "    for i in range(len(possible_states)):\n",
    "        start_t_key =  \"transition: \" + \"START\" + \"+\" + possible_states[i]\n",
    "        feature_expected_counts[start_t_key] = feature_expected_counts.get(start_t_key, 0.0) + np.exp(min(forward_scores[0, i] + backward_scores[0, i] - alpha, threshold))\n",
    "        stop_t_key =  \"transition: \" + possible_states[i] + \"+\"  + \"STOP\"\n",
    "        feature_expected_counts[stop_t_key] = feature_expected_counts.get(stop_t_key, 0.0) + np.exp(min(forward_scores[len(x)-1, i] + backward_scores[len(x)-1, i] - alpha, threshold))\n",
    "    \n",
    "    for i in range(len(possible_states)):\n",
    "        for j in range(len(possible_states)):\n",
    "            t_key =  \"transition: \" + possible_states[i] + \"+\"  + possible_states[j]\n",
    "            t_prob = f.get(t_key, -2**31) \n",
    "            total = 0\n",
    "            for k in range(len(x)-1):\n",
    "                e_key =  \"emission: \" + possible_states[j] + \"+\"  + x[k+1]\n",
    "                e_prob = f.get(e_key, -2**31)\n",
    "\n",
    "                total += np.exp(min(forward_scores[k, i] + backward_scores[k+1, j] + t_prob + e_prob - alpha, threshold))\n",
    "\n",
    "            feature_expected_counts[t_key] = total\n",
    "    \n",
    "    return feature_expected_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_count(x, y, feature_dict):\n",
    "    n = len(x)\n",
    "    feature_count = {}\n",
    "    \n",
    "    for i in range(n):\n",
    "        formatted_word = x[i]\n",
    "        emission_key = \"emission: \"+ y[i] + \"+\" + formatted_word\n",
    "        feature_count[emission_key] = feature_count.get(emission_key, 0) + 1\n",
    "    \n",
    "    updated_y = [\"START\"] + y + [\"STOP\"]\n",
    "    for i in range(1, n+2):\n",
    "        prev_y = updated_y[i-1]\n",
    "        y_i = updated_y[i]\n",
    "        transition_key = \"transition: \" + prev_y + \"+\" + y_i\n",
    "        feature_count[transition_key] = feature_count.get(transition_key, 0) + 1\n",
    "    \n",
    "    return feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(train_dataset, f, possible_states):\n",
    "    feature_gradients = {}\n",
    "    for sentence in train_dataset:\n",
    "        x = [token_tag_pair[0] for token_tag_pair in sentence]\n",
    "        y = [token_tag_pair[1] for token_tag_pair in sentence]\n",
    "        feature_expected_counts = forward_backward(x, f, possible_states)\n",
    "        actual_counts = get_feature_count(x, y, f)\n",
    "        \n",
    "        for k, v in feature_expected_counts.items():\n",
    "            feature_gradients[k] = feature_gradients.get(k, 0) + v\n",
    "            \n",
    "        for k, v in actual_counts.items():\n",
    "            feature_gradients[k] = feature_gradients.get(k, 0) - v\n",
    "\n",
    "    return feature_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running emission: O+the\n",
      "-0.3243840183131397 -0.3243869287425456\n",
      "Running transition: START+O\n",
      "-0.9774762474989984 -0.97748380541465\n",
      "Running transition: O+O\n",
      "-240.81111541818245 -240.81194339196077\n"
     ]
    }
   ],
   "source": [
    "feature_key_checks = ['emission: O+the', 'transition: START+O', 'transition: O+O']\n",
    "feature_gradients = compute_gradients(train_dataset, f, possible_states)\n",
    "loss1 = CRF_loss(train_dataset, f, possible_states)\n",
    "delta = 1e-6\n",
    "\n",
    "for feature_key in feature_key_checks:\n",
    "    print(\"Running\", feature_key)\n",
    "    new_f = f.copy()\n",
    "    new_f[feature_key] += delta\n",
    "\n",
    "    loss2 = CRF_loss(train_dataset, new_f, possible_states)\n",
    "\n",
    "    numerical_gradient = (loss2 - loss1) / delta\n",
    "    analytical_gradient = feature_gradients[feature_key]\n",
    "    print(numerical_gradient, analytical_gradient)\n",
    "    \n",
    "    # SANITY CHECK\n",
    "    assert(abs(numerical_gradient - analytical_gradient) / max(abs(numerical_gradient), 1e-8) <= 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:3546.3917\n",
      "Loss:3200.1376\n",
      "Loss:3123.8846\n",
      "Loss:2983.3788\n",
      "Loss:2848.3104\n",
      "Loss:2644.9568\n",
      "Loss:2479.1922\n",
      "Loss:2428.1791\n",
      "Loss:2280.4901\n",
      "Loss:2181.1284\n",
      "Loss:2131.0115\n",
      "Loss:2086.8457\n",
      "Loss:2055.4427\n",
      "Loss:2021.9711\n",
      "Loss:2007.9220\n",
      "Loss:1976.4204\n",
      "Loss:1957.3419\n",
      "Loss:1946.0982\n",
      "Loss:1931.1227\n",
      "Loss:1927.8104\n",
      "Loss:1919.8086\n",
      "Loss:1914.1922\n",
      "Loss:1904.4882\n",
      "Loss:1894.4842\n",
      "Loss:1881.5888\n",
      "Loss:1875.7357\n",
      "Loss:1865.2601\n",
      "Loss:1861.4945\n",
      "Loss:1858.5552\n",
      "Loss:1854.2032\n",
      "Loss:1850.8877\n",
      "Loss:1847.0880\n",
      "Loss:1845.0712\n",
      "Loss:1843.1217\n",
      "Loss:1838.7683\n",
      "Loss:1837.6898\n",
      "Loss:1836.2430\n",
      "Loss:1835.4419\n",
      "Loss:1834.6623\n",
      "Loss:1833.8303\n",
      "Loss:1830.6794\n",
      "Loss:1829.8910\n",
      "Loss:1829.3293\n",
      "Loss:1828.4382\n",
      "Loss:1827.8615\n",
      "Loss:1827.0591\n",
      "Loss:1826.6538\n",
      "Loss:1825.9795\n",
      "Loss:1825.7165\n",
      "Loss:1825.0844\n",
      "Loss:1824.4885\n",
      "Loss:1823.5524\n",
      "Loss:1822.8798\n",
      "Loss:1822.6207\n",
      "Loss:1822.1026\n",
      "Loss:1821.9470\n",
      "Loss:1821.8232\n",
      "Loss:1821.4850\n",
      "Loss:1821.2550\n",
      "Loss:1820.8130\n",
      "Loss:1820.6290\n",
      "Loss:1820.4848\n",
      "Loss:1820.2341\n",
      "Loss:1820.1481\n",
      "Loss:1820.0869\n",
      "Loss:1820.0153\n",
      "Loss:1819.9618\n",
      "Loss:1819.7801\n",
      "Loss:1819.6250\n",
      "Loss:1819.5383\n",
      "Loss:1819.4791\n",
      "Loss:1819.3862\n",
      "Loss:1819.2935\n",
      "Loss:1819.1733\n",
      "Loss:1819.1418\n",
      "Loss:1819.0817\n",
      "Loss:1818.8769\n",
      "Loss:1818.7567\n",
      "Loss:1818.7164\n",
      "Loss:1818.6454\n",
      "Loss:1818.5697\n",
      "Loss:1818.5229\n",
      "Loss:1818.4840\n",
      "Loss:1818.4374\n",
      "Loss:1818.3451\n",
      "Loss:1818.3040\n",
      "Loss:1818.2735\n",
      "Loss:1818.2614\n",
      "Loss:1818.2483\n",
      "Loss:1818.2266\n",
      "Loss:1818.1991\n",
      "Loss:1818.1852\n",
      "Loss:1818.1650\n",
      "Loss:1818.1419\n",
      "Loss:1818.1284\n",
      "Loss:1818.1078\n",
      "Loss:1818.1051\n",
      "Loss:1818.0918\n",
      "Loss:1818.0895\n",
      "Loss:1818.0870\n",
      "Loss:1818.0809\n",
      "Loss:1818.0736\n",
      "Loss:1818.0631\n",
      "Loss:1818.0573\n",
      "Loss:1818.0447\n",
      "Loss:1818.0374\n",
      "Loss:1818.0315\n",
      "Loss:1818.0264\n",
      "Loss:1818.0220\n",
      "Loss:1818.0170\n",
      "Loss:1818.0134\n",
      "Loss:1818.0086\n",
      "Loss:1818.0055\n",
      "Loss:1818.0004\n",
      "Loss:1817.9945\n",
      "Loss:1817.9928\n",
      "Loss:1817.9905\n",
      "Loss:1817.9891\n",
      "Loss:1817.9865\n",
      "Loss:1817.9858\n",
      "Loss:1817.9843\n",
      "Loss:1817.9824\n",
      "Loss:1817.9809\n",
      "Loss:1817.9782\n",
      "Loss:1817.9758\n",
      "Loss:1817.9741\n",
      "Loss:1817.9723\n",
      "Loss:1817.9707\n",
      "Loss:1817.9695\n",
      "Loss:1817.9686\n",
      "Loss:1817.9675\n",
      "Loss:1817.9662\n",
      "Loss:1817.9655\n",
      "Loss:1817.9648\n",
      "Loss:1817.9643\n",
      "Loss:1817.9637\n",
      "Loss:1817.9632\n",
      "Loss:1817.9625\n",
      "Loss:1817.9620\n",
      "Loss:1817.9619\n",
      "Loss:1817.9613\n",
      "Loss:1817.9611\n",
      "Loss:1817.9606\n",
      "Loss:1817.9603\n",
      "Loss:1817.9600\n",
      "Loss:1817.9596\n",
      "Loss:1817.9593\n",
      "Loss:1817.9591\n",
      "Loss:1817.9588\n",
      "Loss:1817.9585\n",
      "Loss:1817.9581\n",
      "Loss:1817.9580\n",
      "Loss:1817.9577\n",
      "Loss:1817.9576\n",
      "Loss:1817.9574\n",
      "Loss:1817.9571\n",
      "Loss:1817.9570\n",
      "Loss:1817.9568\n",
      "Loss:1817.9567\n",
      "Loss:1817.9567\n",
      "Loss:1817.9565\n",
      "Loss:1817.9564\n",
      "Loss:1817.9563\n",
      "Loss:1817.9562\n",
      "Loss:1817.9562\n",
      "Loss:1817.9561\n",
      "Loss:1817.9559\n",
      "Loss:1817.9559\n",
      "Loss:1817.9559\n",
      "Loss:1817.9558\n",
      "Loss:1817.9558\n",
      "Loss:1817.9557\n",
      "Loss:1817.9557\n",
      "Loss:1817.9556\n",
      "Loss:1817.9556\n",
      "Loss:1817.9556\n",
      "Loss:1817.9555\n",
      "Loss:1817.9554\n",
      "Loss:1817.9554\n",
      "Loss:1817.9553\n",
      "Loss:1817.9553\n",
      "Loss:1817.9552\n",
      "Loss:1817.9552\n",
      "Loss:1817.9552\n",
      "Loss:1817.9552\n",
      "Loss:1817.9551\n",
      "Loss:1817.9551\n",
      "Loss:1817.9551\n",
      "Loss:1817.9551\n",
      "Loss:1817.9551\n",
      "Loss:1817.9551\n",
      "Loss:1817.9550\n",
      "Loss:1817.9550\n",
      "Loss:1817.9550\n",
      "Loss:1817.9550\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "def compute_gradients_with_reg(train_dataset, f, possible_states, eta = 0):\n",
    "    feature_gradients = {}\n",
    "    # for i in range(len(train_labels)):\n",
    "    #     x = train_inputs[i]\n",
    "    #     y = train_labels[i]\n",
    "    for sentence in train_dataset:\n",
    "        x = [token_tag_pair[0] for token_tag_pair in sentence]\n",
    "        y = [token_tag_pair[1] for token_tag_pair in sentence]\n",
    "        feature_expected_counts = forward_backward(x, f, possible_states)\n",
    "        actual_counts = get_feature_count(x, y, f)\n",
    "        \n",
    "        for k, v in feature_expected_counts.items():\n",
    "            feature_gradients[k] = feature_gradients.get(k, 0) + v\n",
    "            \n",
    "        for k, v in actual_counts.items():\n",
    "            feature_gradients[k] = feature_gradients.get(k, 0) - v\n",
    "\n",
    "    for k, v in f.items():\n",
    "        feature_gradients[k] =feature_gradients.get(k,0) + 2*eta*f[k]\n",
    "\n",
    "    return feature_gradients\n",
    "\n",
    "def compute_crf_loss_with_reg(train_dataset, f, possible_states, eta=0):\n",
    "    loss = 0\n",
    "\n",
    "    for sentence in train_dataset:\n",
    "        x = [token_tag_pair[0] for token_tag_pair in sentence]\n",
    "        y = [token_tag_pair[1] for token_tag_pair in sentence]\n",
    "    # for i in range(len(input_sequences)):\n",
    "        first_term = calculate_score(x, y, f)\n",
    "        _, alpha = forward_algorithm(x, f, possible_states)\n",
    "        loss += (first_term - alpha) * -1\n",
    "\n",
    "    reg_loss = 0\n",
    "    for f_key in f:\n",
    "        reg_loss += f[f_key]**2\n",
    "    reg_loss = eta * reg_loss\n",
    "    loss += reg_loss\n",
    "    return loss\n",
    "\n",
    "def callbackF(w):\n",
    "    '''\n",
    "    This function will be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "        w: weights, numpy array\n",
    "    '''\n",
    "    loss = compute_crf_loss_with_reg(train_dataset,f,possible_states,0.1) \n",
    "    print('Loss:{0:.4f}'.format(loss))\n",
    "\n",
    "def get_loss_grad(w, *args): \n",
    "    '''\n",
    "    This function will be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "        w: weights, numpy array\n",
    "    Returns:\n",
    "        loss: loss, float\n",
    "        grads: gradients, numpy array\n",
    "    '''\n",
    "    train_dataset, f, possible_states = args\n",
    "    for i,k in enumerate(f.keys()):\n",
    "        f[k] = w[i]\n",
    "    \n",
    "    loss = compute_crf_loss_with_reg(train_dataset,f,possible_states,0.1)\n",
    "    grads = compute_gradients_with_reg(train_dataset,f,possible_states,0.1)\n",
    "    np_grads = np.zeros(len(f))\n",
    "    for i,k in enumerate(f.keys()):\n",
    "        np_grads[i] = grads[k]\n",
    "    grads = np_grads\n",
    "    return loss, grads\n",
    "\n",
    "init_w = np.zeros(len(f))\n",
    "result = fmin_l_bfgs_b(get_loss_grad, init_w, args=(train_dataset,f,possible_states), pgtol=0.01, callback=callbackF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 3809 tokens with 210 phrases; found: 229 phrases; correct: 79.\n",
      "accuracy:  35.02%; (non-O)\n",
      "accuracy:  90.39%; precision:  34.50%; recall:  37.62%; FB1:  35.99\n",
      "         negative: precision:  29.27%; recall:  18.46%; FB1:  22.64  41\n",
      "          neutral: precision:   0.00%; recall:   0.00%; FB1:   0.00  27\n",
      "         positive: precision:  41.61%; recall:  48.91%; FB1:  44.97  161\n",
      "(34.49781659388647, 37.61904761904762, 35.99088838268793)\n"
     ]
    }
   ],
   "source": [
    "weight, loss, dictionary = result\n",
    "\n",
    "for idx, key in enumerate(f.keys()):\n",
    "    f[key] = weight[idx]\n",
    "\n",
    "decode(\"dataset/dev.in\", possible_states, f, 'dataset/dev.p4.out')\n",
    "g_tags, p_tags = eval('dataset/dev.p4.out', 'dataset/dev.out')\n",
    "print(evaluate(g_tags,p_tags,verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_count_p5(train_dataset):\n",
    "    feature_counts = {}\n",
    "    emission_count = {}\n",
    "    transition_count = {}\n",
    "    uni_count = {}\n",
    "    bi_count = {}\n",
    "    state_count = {}\n",
    "    start_state = \"START\"\n",
    "    stop_state = \"STOP\"\n",
    "\n",
    "    for sentence in train_dataset:\n",
    "        x = [token_tag_pair[0] for token_tag_pair in sentence]\n",
    "        y = [token_tag_pair[1] for token_tag_pair in sentence]\n",
    "\n",
    "        n = len(x)\n",
    "\n",
    "        # START state\n",
    "        if uni_count.get(start_state) == None:\n",
    "            uni_count[start_state] = {}\n",
    "        uni_count[start_state][x[0]] = uni_count[start_state].get(x[0], 0) + 1\n",
    "\n",
    "        state_count[start_state] = state_count.get(start_state, 0) + 1\n",
    "\n",
    "        # STOP state\n",
    "        if transition_count.get(y[n-1]) == None:\n",
    "            transition_count[y[n-1]] = {}\n",
    "        transition_count[y[n-1]][stop_state] = transition_count[y[n-1]].get(stop_state, 0) + 1\n",
    "        \n",
    "        if uni_count.get(stop_state) == None:\n",
    "            uni_count[stop_state] = {}\n",
    "        uni_count[stop_state][x[n-1]] = uni_count[stop_state].get(x[n-1], 0) + 1\n",
    "\n",
    "        state_count[stop_state] = state_count.get(stop_state, 0) + 1\n",
    "\n",
    "        for i in range(n):\n",
    "\n",
    "            # First word\n",
    "            if i == 0:\n",
    "                if emission_count.get(y[i]) == None:\n",
    "                    emission_count[y[i]] = {}\n",
    "                emission_count[y[i]][x[i]] = emission_count[y[i]].get(x[i], 0) + 1\n",
    "                        \n",
    "                if n>1:\n",
    "                    if uni_count.get(y[i]) == None:\n",
    "                        uni_count[y[i]] = {}\n",
    "                    uni_count[y[i]][x[i+1]] = uni_count[y[i]].get(x[i+1], 0) + 1\n",
    "\n",
    "                if transition_count.get(start_state) == None:\n",
    "                    transition_count[start_state] = {}\n",
    "                transition_count[start_state][y[i]] = transition_count[start_state].get(y[i], 0) + 1\n",
    "\n",
    "                if bi_count.get((start_state, y[i])) == None:\n",
    "                    bi_count[(start_state, y[i])] = {}\n",
    "                bi_count[(start_state, y[i])][x[i]] = bi_count[(start_state, y[i])].get(x[i], 0) + 1\n",
    "\n",
    "                state_count[(start_state, y[i])] = state_count.get((start_state, y[i]), 0) + 1                \n",
    "\n",
    "            # Last word\n",
    "            elif i == n-1:\n",
    "                if emission_count.get(y[i]) == None:\n",
    "                    emission_count[y[i]] = {}\n",
    "                emission_count[y[i]][x[i]] = emission_count[y[i]].get(x[i], 0) + 1\n",
    "\n",
    "                if uni_count.get(y[i]) == None:\n",
    "                    uni_count[y[i]] = {}\n",
    "                uni_count[y[i]][x[i-1]] = uni_count[y[i]].get(x[i-1], 0) + 1\n",
    "\n",
    "                if transition_count.get(y[i-1]) == None:\n",
    "                    transition_count[y[i-1]] = {}\n",
    "                transition_count[y[i-1]][y[i]] = transition_count[y[i-1]].get(y[i], 0) + 1\n",
    "\n",
    "                if bi_count.get((y[i-1], y[i])) == None:\n",
    "                    bi_count[(y[i-1], y[i])] = {}\n",
    "                bi_count[(y[i-1], y[i])][x[i]] = bi_count[(y[i-1], y[i])].get(x[i], 0) + 1\n",
    "\n",
    "                state_count[(y[i-1], y[i])] = state_count.get((y[i-1], y[i]), 0) + 1\n",
    "\n",
    "            # Middle words\n",
    "            else:\n",
    "                if emission_count.get(y[i]) == None:\n",
    "                    emission_count[y[i]] = {}\n",
    "                emission_count[y[i]][x[i]] = emission_count[y[i]].get(x[i], 0) + 1\n",
    "\n",
    "                if uni_count.get(y[i]) == None:\n",
    "                    uni_count[y[i]] = {}\n",
    "                uni_count[y[i]][x[i-1]] = uni_count[y[i]].get(x[i-1], 0) + 1\n",
    "\n",
    "                if uni_count.get(y[i]) == None:\n",
    "                    uni_count[y[i]] = {}\n",
    "                uni_count[y[i]][x[i+1]] = uni_count[y[i]].get(x[i+1], 0) + 1\n",
    "\n",
    "                if transition_count.get(y[i-1]) == None:\n",
    "                    transition_count[y[i-1]] = {}\n",
    "                transition_count[y[i-1]][y[i]] = transition_count[y[i-1]].get(y[i], 0) + 1\n",
    "\n",
    "                if bi_count.get((y[i-1], y[i])) == None:\n",
    "                    bi_count[(y[i-1], y[i])] = {}\n",
    "                bi_count[(y[i-1], y[i])][x[i]] = bi_count[(y[i-1], y[i])].get(x[i], 0) + 1\n",
    "\n",
    "                state_count[(y[i-1], y[i])] = state_count.get((y[i-1], y[i]), 0) + 1\n",
    "\n",
    "            state_count[y[i]] = state_count.get(y[i], 0) + 1\n",
    "\n",
    "        state_count[(y[n-1], stop_state)] = state_count.get((y[n-1], stop_state), 0) + 1\n",
    "\n",
    "    return emission_count, transition_count, uni_count, bi_count, state_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict_p5(emission_count, transition_count, uni_count, bi_count, state_count):\n",
    "    f = {}\n",
    "    for tag, tokens in emission_count.items():\n",
    "        for token, e_count in tokens.items():\n",
    "            key = \"emission: \" + tag + '+' + token\n",
    "            e_prob = np.log(e_count/state_count[tag])\n",
    "            f[key] = e_prob\n",
    "\n",
    "    for prev_tag, next_tags in transition_count.items():\n",
    "        for next_tag, t_count in next_tags.items():\n",
    "            key = \"transition: \" + prev_tag + '+' + next_tag\n",
    "            t_prob = np.log(t_count/state_count[prev_tag])\n",
    "            f[key] = t_prob\n",
    "\n",
    "    for tag, tokens in uni_count.items():\n",
    "        for token, u_count in tokens.items():\n",
    "            key = \"unigram: \" + tag + '+' + token\n",
    "            u_prob = np.log(u_count/state_count[tag])\n",
    "            f[key] = u_prob\n",
    "\n",
    "    for tag, tokens in bi_count.items():\n",
    "        for token, b_count in tokens.items():\n",
    "            key = \"bigram: \" + tag[0] + '+' + tag[1] + '+' + token\n",
    "            b_prob = np.log(b_count/state_count[tag])\n",
    "            f[key] = b_prob\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_count, transition_count, uni_count, bi_count, state_count = get_feature_count_p5(train_dataset)\n",
    "f_p5 = get_feature_dict_p5(emission_count, transition_count, uni_count, bi_count, state_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_p5(x, possible_states, f, default_index=0):\n",
    "    n = len(x)\n",
    "    d = len(possible_states)\n",
    "    scores = np.full((n, d), -np.inf)\n",
    "    bp = np.full((n, d), default_index, dtype=np.int32)\n",
    "\n",
    "    for i in range(len(possible_states)):\n",
    "        t_key = \"transition: START\"+possible_states[i]\n",
    "        e_key = \"emission: \"+possible_states[i]+\"+\"+x[0]\n",
    "        b_key = \"bigram: START\"+possible_states[i]+\"+\"+x[0]\n",
    "\n",
    "        t_prob = f.get(t_key, -2**31)\n",
    "        e_prob = f.get(e_key, -2**31)\n",
    "        b_prob = f.get(b_key, -2**31)\n",
    "        \n",
    "        if n > 1:\n",
    "            u_key = \"unigram: \"+possible_states[i]+\"+\"+x[1]\n",
    "            u_prob = f.get(u_key, -2**31)\n",
    "            scores[0, i] = t_prob + e_prob + u_prob + b_prob\n",
    "\n",
    "        else:\n",
    "            scores[0, i] = t_prob + e_prob + b_prob\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        for k in range(len(possible_states)):\n",
    "            for j in range(len(possible_states)):\n",
    "                t_key = \"transition: \"+possible_states[k]+\"+\"+possible_states[j]\n",
    "                e_key = \"emission: \"+possible_states[j]+\"+\"+x[i]\n",
    "                u_key1 = \"unigram: \"+possible_states[j]+\"+\"+x[i-1]\n",
    "                b_key = \"bigram: \"+possible_states[k]+possible_states[j]+\"+\"+x[i]\n",
    "\n",
    "                t_prob = f.get(t_key, -2**31)\n",
    "                e_prob = f.get(e_key, -2**31)\n",
    "                u_prob1 = f.get(u_key1, -2**31)\n",
    "                b_prob = f.get(b_key, -2**31)\n",
    "\n",
    "                if i != n-1:\n",
    "                    u_key2 = \"unigram: \"+possible_states[j]+\"+\"+x[i+1]\n",
    "                    u_prob2 = f.get(u_key2, -2**31)\n",
    "                    overall_score = e_prob + t_prob + + u_prob1 + u_prob2 + b_prob + scores[i-1, k]\n",
    "\n",
    "                else:\n",
    "                    overall_score = e_prob + t_prob + + u_prob1 + b_prob + scores[i-1, k]\n",
    "\n",
    "                if overall_score > scores[i, j]:\n",
    "                    scores[i, j] = overall_score\n",
    "                    bp[i,j] = k\n",
    "    \n",
    "    highest_score = -2**31\n",
    "    highest_bp = default_index\n",
    "    for i in range(len(possible_states)):\n",
    "        t_key = \"transition: \"+possible_states[i]+\"+STOP\"\n",
    "        t_prob = f.get(t_key, -2**31)\n",
    "\n",
    "        overall_score = t_prob + scores[n-1, i]\n",
    "        \n",
    "        if overall_score > highest_score:\n",
    "            highest_score = overall_score\n",
    "            highest_bp = i\n",
    "    \n",
    "    result = [possible_states[highest_bp]]\n",
    "    prev_bp = highest_bp\n",
    "    for i in range(n-1, 0, -1):\n",
    "        prev_bp = bp[i, prev_bp]\n",
    "        output = possible_states[prev_bp]\n",
    "        result = [output] + result\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_p5(path, states, f, output_filename):\n",
    "    default_index = states.index('O')\n",
    "    sentences = list()\n",
    "\n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "        sentence = list()\n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()   \n",
    "            \n",
    "            if(len(formatted_line) ==0):\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            sentence.append(formatted_line)\n",
    "\n",
    "    with open(output_filename, \"w\") as wf:\n",
    "        for sentence in sentences:\n",
    "            pred_sentence = viterbi_p5(sentence, states, f, default_index)        \n",
    "            for i in range(len(sentence)):\n",
    "                wf.write(sentence[i] + \" \" + pred_sentence[i] + \"\\n\")\n",
    "                \n",
    "            wf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 3809 tokens with 210 phrases; found: 218 phrases; correct: 100.\n",
      "accuracy:  41.08%; (non-O)\n",
      "accuracy:  92.94%; precision:  45.87%; recall:  47.62%; FB1:  46.73\n",
      "         negative: precision:  35.71%; recall:  23.08%; FB1:  28.04  42\n",
      "          neutral: precision:  16.67%; recall:  37.50%; FB1:  23.08  18\n",
      "         positive: precision:  51.90%; recall:  59.85%; FB1:  55.59  158\n",
      "(45.87155963302752, 47.61904761904761, 46.728971962616825)\n"
     ]
    }
   ],
   "source": [
    "decode_p5(\"dataset/dev.in\", possible_states, f_p5, 'dataset/dev.p5.out')\n",
    "g_tags, p_tags = eval('dataset/dev.p5.out', 'dataset/dev.out')\n",
    "print(evaluate(g_tags,p_tags,verbose=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ml_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "868b7fb96f33f69c77b53090836525b77aeeeabc65f63ab10b8eda185580798a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
