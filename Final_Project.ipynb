{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_file(filename):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # Split the entire file into sentences. Output: List of sentences\n",
    "    sentences = file_content.strip().split('\\n\\n')\n",
    "\n",
    "    # Split each sentence into their token_tag pair\n",
    "    # Output: List of sentences. Each sentence is a list of token_tag_pair\n",
    "    token_tag_pairs = [i.split('\\n') for i in sentences]\n",
    "\n",
    "    # Separate each token_tag_pair into a list of [token, tag].\n",
    "    # Output: [[[token, tag], [token, tag], ...], [[token, tag], [token, tag], ...], ...]\n",
    "    for idx, sentence in enumerate(token_tag_pairs):\n",
    "        token_tags = [i.rsplit(' ', maxsplit=1) for i in sentence]\n",
    "        token_tag_pairs[idx] = token_tags\n",
    "\n",
    "    return token_tag_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = './dataset/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = read_train_file(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_count = {}\n",
    "transition_count = {}\n",
    "state_count = {}\n",
    "possible_states = []\n",
    "\n",
    "transition_count['START'] = {}\n",
    "for sentence in train_dataset:\n",
    "    prev_state = None\n",
    "    \n",
    "    for token, tag in sentence:\n",
    "        if tag not in possible_states:\n",
    "            possible_states.append(tag)\n",
    "\n",
    "        if emission_count.get(token) == None:\n",
    "            emission_count[token] = {}\n",
    "        \n",
    "        emission_count[token][tag] = emission_count[token].get(tag, 0) + 1\n",
    "\n",
    "        if prev_state != None:\n",
    "            if transition_count.get(prev_state) == None:\n",
    "                transition_count[prev_state] = {}\n",
    "            transition_count[prev_state][tag] = transition_count[prev_state].get(tag, 0) + 1\n",
    "\n",
    "        else:\n",
    "            transition_count['START'][tag] = transition_count['START'].get(tag, 0) + 1\n",
    "            state_count['START'] = state_count.get('START', 0) + 1\n",
    "\n",
    "        state_count[tag] = state_count.get(tag, 0) + 1\n",
    "        prev_state = tag\n",
    "\n",
    "    transition_count[prev_state]['STOP'] = transition_count[prev_state].get('STOP', 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = {}\n",
    "\n",
    "for token, tags in emission_count.items():\n",
    "    for tag, e_count in tags.items():\n",
    "        key = \"emission: \" + tag + '+' + token\n",
    "        e_prob = np.log(e_count/state_count[tag])\n",
    "        f[key] = e_prob\n",
    "\n",
    "for prev_tag, next_tags in transition_count.items():\n",
    "    for next_tag, t_count in next_tags.items():\n",
    "        key = \"transition: \" + prev_tag + '+' + next_tag\n",
    "        t_prob = np.log(t_count/state_count[prev_tag])\n",
    "        f[key] = t_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition: START+O -0.06165959394788234\n",
      "transition: START+B-negative -4.539564428890097\n",
      "transition: START+B-positive -3.153270067770207\n",
      "transition: START+B-neutral -5.0503900526560885\n",
      "transition: O+O -0.1469289780609056\n",
      "transition: O+B-positive -3.100585479223317\n",
      "transition: O+B-negative -4.235764913309846\n",
      "transition: O+STOP -2.5966054955940074\n",
      "transition: O+B-neutral -5.94191062981491\n",
      "transition: B-positive+O -0.3572728512809094\n",
      "transition: B-positive+I-positive -1.2540189870820944\n",
      "transition: B-positive+STOP -4.252688120309395\n",
      "transition: B-positive+B-positive -7.0859014643656115\n",
      "transition: B-negative+O -0.2344506222289012\n",
      "transition: B-negative+I-negative -1.6041608553332567\n",
      "transition: B-negative+STOP -4.836281906951478\n",
      "transition: I-positive+O -0.5419771288708248\n",
      "transition: I-positive+I-positive -0.8965221465517323\n",
      "transition: I-positive+STOP -4.564348191467836\n",
      "transition: B-neutral+O -0.23293155768037255\n",
      "transition: B-neutral+I-neutral -1.7788560643921472\n",
      "transition: B-neutral+STOP -3.245193133185574\n",
      "transition: I-neutral+I-neutral -0.7339691750802004\n",
      "transition: I-neutral+O -0.6539264674066639\n",
      "transition: I-negative+O -0.5134537461722601\n",
      "transition: I-negative+I-negative -0.9123614537342655\n"
     ]
    }
   ],
   "source": [
    "for key in f.keys():\n",
    "    if key.startswith('transition:'):\n",
    "        print(key, f[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "calculate_score(x,y):\n",
    "Helps to calulate the score for a given pair of input and output sequence pair (x,y)\n",
    "Based on 2 features, emission and transition\n",
    "\n",
    "Parameters:\n",
    "x: List of tokens, e.g. x = x1, x2, ..., xn     Type: list[str]\n",
    "y: List of tokens, e.g. y = y1, y2, ..., yn     Type: list[str]\n",
    "f: Dictionary of feature weights                   Type: Dict{features: weights}\n",
    "'''\n",
    "\n",
    "def calculate_score(x,y,f):\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "    feature_count = {}\n",
    "\n",
    "    prev_tag = 'START'\n",
    "    score = 0\n",
    "\n",
    "    length = len(x)\n",
    "    for i in range(length):\n",
    "        e_key = \"emission: \" + y[i] + '+' + x[i]\n",
    "        t_key = \"transition: \" + prev_tag + '+' + y[i]\n",
    "\n",
    "        if e_key in f.keys():\n",
    "            feature_count[e_key] = feature_count.get(e_key, 0) + 1\n",
    "\n",
    "        if t_key in f.keys():\n",
    "            feature_count[t_key] = feature_count.get(t_key, 0) + 1\n",
    "\n",
    "        prev_tag = y[i]\n",
    "        \n",
    "    t_key = \"transition: \" + prev_tag + '+' + 'STOP'\n",
    "    if t_key in f.keys():\n",
    "            feature_count[t_key] = feature_count.get(t_key, 0) + 1\n",
    "\n",
    "    for feature, count in feature_count.items():\n",
    "        score += f[feature] * count\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence, f):\n",
    "        # BASE CASE\n",
    "        scores = {\n",
    "            0: {\n",
    "                'START' : 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        index = 1\n",
    "\n",
    "        # Forward Algorithm - From START to index N\n",
    "        for token in sentence:\n",
    "            scores[index] = {}\n",
    "\n",
    "            for state in possible_states:\n",
    "                state_scores = {}\n",
    "\n",
    "                for prev_tag in scores[index-1].keys():\n",
    "                    e_key = \"emission: \" + state + '+' + token\n",
    "                    t_key = \"transition: \" + prev_tag + '+' + token\n",
    "                    e_prob = f.get(e_key, float('-inf'))\n",
    "                    t_prob = f.get(t_key, float('-inf'))\n",
    "\n",
    "                    # t_prob = self._calculate_transition_MLE(prev_tag, state)\n",
    "                    # e_prob = self._calculate_emission_MLE_UNK(token, state)\n",
    "\n",
    "                    if t_prob != float('-inf') and e_prob != float('-inf'):\n",
    "                        state_scores[prev_tag] = \\\n",
    "                            scores[index-1][prev_tag] + \\\n",
    "                            t_prob + \\\n",
    "                            e_prob\n",
    "                    else:\n",
    "                        state_scores[prev_tag] = float('-inf')\n",
    "\n",
    "                best_score = max(state_scores.values())\n",
    "                scores[index][state] = best_score\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        # Forward Algorithm - From index N to STOP\n",
    "        state_scores = {}\n",
    "        for prev_tag in scores[index-1].keys():\n",
    "            t_key = \"transition: \" + prev_tag + '+' + 'STOP'\n",
    "            t_prob = f.get(t_key, 0)\n",
    "            if t_prob > 0:\n",
    "                state_scores[prev_tag] = scores[index-1][prev_tag] + np.log(t_prob)\n",
    "            else:\n",
    "                state_scores[prev_tag] = float('-inf')\n",
    "\n",
    "        y_n = max(state_scores, key=state_scores.get)\n",
    "        prediction_reversed = [y_n]\n",
    "\n",
    "        # Backtracking Algorithm\n",
    "        for n in reversed(range(1,index)):\n",
    "            state_scores = {}\n",
    "\n",
    "            for state in scores[n-1].keys():\n",
    "                t_key = \"transition: \" + state + '+' + prediction_reversed[-1]\n",
    "                t_prob = f.get(t_key, 0)\n",
    "                \n",
    "                # t_prob = self._calculate_transition_MLE(state, prediction_reversed[-1])\n",
    "\n",
    "                if t_prob > 0:\n",
    "                    state_scores[state] = scores[n-1][state] + np.log(t_prob)\n",
    "\n",
    "            if all(prob == float('-inf') for prob in state_scores.values()):\n",
    "                prediction_reversed.append('O')\n",
    "            else:\n",
    "                best_state = max(state_scores, key=state_scores.get)\n",
    "                prediction_reversed.append(best_state)\n",
    "\n",
    "        prediction = []\n",
    "        prediction_reversed.reverse()\n",
    "\n",
    "        for idx, token in enumerate(sentence):\n",
    "            prediction.append([token, prediction_reversed[idx+1]])\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Loved', 'O'], ['it', 'O']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi([\"Loved\",\"it\"], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_count(train_dataset):\n",
    "    feature_counts = {}\n",
    "    for sentence in train_dataset:\n",
    "        x = [token_tag_pair[0] for token_tag_pair in sentence]\n",
    "        y = [token_tag_pair[1] for token_tag_pair in sentence]\n",
    "\n",
    "        n = len(x)\n",
    "\n",
    "        for i in range(n):\n",
    "            e_key1 = \"emission: \" + y[i] + \"+\" + x[i]\n",
    "            e_key2 = \"emission: \" + y[i] + \"+\" + x[i-1]\n",
    "            e_key3 = \"emission: \" + y[i] + \"+\" + x[i+1]\n",
    "            t_key1 = \"transition: \" + y[i-1] + y[i]\n",
    "            combined_key1 = \"transition: \" + y[i-1] + y[i] + x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of possible features: \n",
    "# triple transitions: y + y + y\n",
    "\n",
    "def triple_transition_counts(train_dataset):\n",
    "    triple_transition_count = {}\n",
    "    for sentence in train_dataset:\n",
    "        for i in range(1, len(sentence) - 1):\n",
    "            start_token = sentence[i-1][1]\n",
    "            mid_token = sentence[i][1]\n",
    "            end_token = sentence[i+1][1]\n",
    "\n",
    "            triple_transition_count[start_token] = triple_transition_count.get(start_token, {})\n",
    "            triple_transition_count[start_token][mid_token] = triple_transition_count[start_token].get(mid_token, {})\n",
    "            triple_transition_count[start_token][mid_token][end_token] = triple_transition_count[start_token][mid_token].get(end_token, 0) + 1\n",
    "\n",
    "    return triple_transition_count\n",
    "\n",
    "def triple_transition_probabilities(triple_transition_counts, transition_count, f):\n",
    "    for start_token, mid_tokens_counts in triple_transition_counts.items():\n",
    "        for mid_token, end_tokens_counts in mid_tokens_counts.items():\n",
    "            for end_token, tt_count in end_tokens_counts.items():\n",
    "                tt_key = \"triple_transition: \" + start_token + '+' + mid_token + '+' + end_token\n",
    "                t_key = \"transition: \" + start_token + '+' + mid_token\n",
    "                f[tt_key] = tt_count / transition_count[t_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-negative': {'O': {'O': 1}},\n",
      " 'B-positive': {'O': {'O': 1}},\n",
      " 'O': {'B-negative': {'O': 1},\n",
      "       'B-positive': {'O': 1},\n",
      "       'O': {'B-negative': 1, 'B-positive': 1, 'O': 7}}}\n"
     ]
    }
   ],
   "source": [
    "pprint(triple_transition_counts([train_dataset[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "calculate_score_part6(x,y):\n",
    "Helps to calulate the score for a given pair of input and output sequence pair (x,y)\n",
    "Based on 2 features, emission and transition\n",
    "\n",
    "Parameters:\n",
    "x: List of tokens, e.g. x = x1, x2, ..., xn     Type: list[str]\n",
    "y: List of tokens, e.g. y = y1, y2, ..., yn     Type: list[str]\n",
    "f: Dictionary of feature weights                   Type: Dict{features: weights}\n",
    "'''\n",
    "\n",
    "def calculate_score(x,y,f):\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "    feature_count = {}\n",
    "\n",
    "    prev_2_tag = None\n",
    "    prev_tag = 'START'\n",
    "    score = 0\n",
    "\n",
    "    length = len(x)\n",
    "    for i in range(length):\n",
    "        e_key = \"emission: \" + y[i] + '+' + x[i]\n",
    "        t_key = \"transition: \" + prev_tag + '+' + y[i]\n",
    "\n",
    "        if e_key in f.keys():\n",
    "            feature_count[e_key] = feature_count.get(e_key, 0) + 1\n",
    "\n",
    "        if t_key in f.keys():\n",
    "            feature_count[t_key] = feature_count.get(t_key, 0) + 1\n",
    "        \n",
    "        if i > 1 and i < length - 1:\n",
    "            tt_key = \"triple_transition: \" + prev_2_tag + '+' + prev_tag + '+' + y[i]\n",
    "            if tt_key in f.keys():\n",
    "                feature_count[tt_key] = feature_count.get(tt_key, 0) + 1\n",
    "        prev_2_tag = prev_tag\n",
    "        prev_tag = y[i]\n",
    "\n",
    "        \n",
    "    t_key = \"transition: \" + prev_tag + '+' + 'STOP'\n",
    "    if t_key in f.keys():\n",
    "        feature_count[t_key] = feature_count.get(t_key, 0) + 1\n",
    "\n",
    "    tt_key = \"triple_transition: \" + prev_2_tag + '+' + prev_tag + '+' + 'STOP'\n",
    "    if tt_key in f.keys():\n",
    "        feature_count[tt_key] = feature_count.get(tt_key, 0) + 1\n",
    "    \n",
    "    for feature, count in feature_count.items():\n",
    "        score += f[feature] * count\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnoldlim/venvs/ml_venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "token_tags = [token_tag for sent in train_dataset for token_tag in sent]\n",
    "train_tokens = [[token for token, tag in sent] for sent in train_dataset]\n",
    "train_tags= [[tag for token, tag in sent] for sent in train_dataset]\n",
    "\n",
    "def build_vocab(words):\n",
    "    counter = Counter()\n",
    "    for word_lst in words:\n",
    "        word_lst = ['START'] + word_lst + ['STOP']\n",
    "        counter.update(word_lst)\n",
    "    sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "    return vocab(ordered_dict, specials=('START', 'STOP', 'PAD'))\n",
    "\n",
    "def build_data(token_vocab, tag_vocab, train_dataset=train_dataset):\n",
    "    data = []\n",
    "    for sent in train_dataset:\n",
    "        token_tensor = torch.LongTensor([token_vocab[token] for token, tag in sent])\n",
    "        tag_tensor = torch.LongTensor([tag_vocab[tag] for token, tag in sent])\n",
    "        \n",
    "        # tag_tensor = F.one_hot(tag_tensor, num_classes=len(tag_vocab))\n",
    "        data.append((token_tensor, tag_tensor))\n",
    "    return data\n",
    "\n",
    "train_vocab, train_tags_vocab = build_vocab(train_tokens), build_vocab(train_tags)\n",
    "train_vocab.set_default_index(train_vocab['a'])\n",
    "train_tags_vocab.set_default_index(train_tags_vocab['PAD'])\n",
    "train_data = build_data(train_vocab, train_tags_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnoldlim/venvs/ml_venv/lib/python3.8/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "TAGS = ['O', 'B-positive', 'I-positive', 'B-negative', 'I-negative', 'B-neutral', 'I-neutral']\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LENGTH = 75\n",
    "START_TOKEN_IDX, STOP_TOKEN_IDX, PAD_TOKEN_IDX = train_vocab['START'], train_vocab['STOP'], train_vocab['PAD']\n",
    "START_TAG_IDX, STOP_TAG_IDX, PAD_TAG_IDX = train_tags_vocab['START'], train_tags_vocab['STOP'], train_tags_vocab['PAD']\n",
    "\n",
    "def process_sentence(sent_tensor, start_idx=START_TOKEN_IDX, stop_idx=STOP_TOKEN_IDX, pad_idx=PAD_TOKEN_IDX):\n",
    "    sent_tensor = torch.cat([torch.tensor([start_idx]), sent_tensor, torch.tensor([stop_idx])])\n",
    "    if sent_tensor.shape[0] < SEQ_LENGTH:\n",
    "        sent_tensor = torch.cat([sent_tensor, torch.tensor([pad_idx] * (SEQ_LENGTH - sent_tensor.shape[0]))])\n",
    "    return sent_tensor\n",
    "\n",
    "def process_text_data(train_data):\n",
    "    token_lens = [len(token_tensor) for token_tensor, tag_tensor in train_data]\n",
    "\n",
    "    padded_token_tensors = [process_sentence(token_tensor, START_TOKEN_IDX, STOP_TOKEN_IDX, PAD_TOKEN_IDX) for token_tensor, tag_tensor in train_data]\n",
    "    padded_tag_tensors = [F.one_hot(process_sentence(tag_tensor, START_TAG_IDX, STOP_TAG_IDX, PAD_TAG_IDX), num_classes=len(train_tags_vocab)) for token_tensor, tag_tensor in train_data]\n",
    "    return [(token_tensor, tag_tensor) for token_tensor, tag_tensor in zip(padded_token_tensors, padded_tag_tensors)]\n",
    "\n",
    "processed_data = process_text_data(train_data)\n",
    "data_size = len(processed_data)\n",
    "processed_train_data = processed_data[:int(data_size * 0.8)]\n",
    "processed_val_data = processed_data[int(data_size * 0.8):]\n",
    "train_dataloader = DataLoader(processed_train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(processed_val_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSA_model(nn.Module):\n",
    "    def __init__(self, vocab_size, num_tags, embedding_dim, hidden_dim, n_layers=3):\n",
    "        super(ABSA_model, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=True, batch_first=True, dropout=0.3)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, num_tags*SEQ_LENGTH)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_tags = num_tags\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(x.shape[0], SEQ_LENGTH, self.num_tags, SEQ_LENGTH)\n",
    "        x = self.sigmoid(x)\n",
    "        return x[:, :, :, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABSA_model(\n",
      "  (embedding): Embedding(3977, 100)\n",
      "  (lstm): LSTM(100, 100, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (linear): Linear(in_features=200, out_features=750, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ABSA_model(vocab_size=len(train_vocab), num_tags=len(train_tags_vocab), embedding_dim=100, hidden_dim=100, n_layers=2)\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def train(model, optimizer, criterion, train_dataloader, epochs=250, early_stopping=5):\n",
    "    early_stopping_losses = []\n",
    "    models = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for i, (token_batch, tag_batch) in enumerate(train_dataloader):\n",
    "            token_batch, tag_batch = token_batch.to(device), tag_batch.to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(token_batch)\n",
    "            tag_batch = tag_batch.to(device, dtype=torch.float32)\n",
    "            loss = criterion(output, tag_batch)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss /= len(train_dataloader)\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        for i, (token_batch, tag_batch) in enumerate(val_dataloader):\n",
    "            token_batch, tag_batch = token_batch.to(device), tag_batch.to(device)\n",
    "            output = model(token_batch)\n",
    "            tag_batch = tag_batch.to(device, dtype=torch.float32)\n",
    "            loss = criterion(output, tag_batch)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        print('Epoch: {}/{}'.format(epoch, epochs), ' Loss: {:.4f}'.format(train_loss), ' Val Loss: {:.4f}'.format(val_loss))\n",
    "        \n",
    "        # early stoppping\n",
    "        models.append(copy.deepcopy(model))\n",
    "        early_stopping_losses.append(val_loss)\n",
    "        if len(early_stopping_losses) > 10:\n",
    "            models.pop(0)\n",
    "            early_stopping_losses.pop(0)\n",
    "        if early_stopping_losses[-1] > early_stopping_losses[0]:\n",
    "            print('Early stopping at epoch: {}'.format(epoch))\n",
    "            model = models[np.argmax(early_stopping_losses)]\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/250  Loss: 0.1716  Val Loss: 0.0403\n",
      "Epoch: 1/250  Loss: 0.0314  Val Loss: 0.0255\n",
      "Epoch: 2/250  Loss: 0.0236  Val Loss: 0.0202\n",
      "Epoch: 3/250  Loss: 0.0192  Val Loss: 0.0167\n",
      "Epoch: 4/250  Loss: 0.0169  Val Loss: 0.0153\n",
      "Epoch: 5/250  Loss: 0.0155  Val Loss: 0.0138\n",
      "Epoch: 6/250  Loss: 0.0142  Val Loss: 0.0128\n",
      "Epoch: 7/250  Loss: 0.0129  Val Loss: 0.0120\n",
      "Epoch: 8/250  Loss: 0.0117  Val Loss: 0.0110\n",
      "Epoch: 9/250  Loss: 0.0103  Val Loss: 0.0101\n",
      "Epoch: 10/250  Loss: 0.0092  Val Loss: 0.0097\n",
      "Epoch: 11/250  Loss: 0.0083  Val Loss: 0.0092\n",
      "Epoch: 12/250  Loss: 0.0075  Val Loss: 0.0093\n",
      "Epoch: 13/250  Loss: 0.0068  Val Loss: 0.0089\n",
      "Epoch: 14/250  Loss: 0.0062  Val Loss: 0.0092\n",
      "Epoch: 15/250  Loss: 0.0056  Val Loss: 0.0098\n",
      "Epoch: 16/250  Loss: 0.0052  Val Loss: 0.0093\n",
      "Epoch: 17/250  Loss: 0.0048  Val Loss: 0.0096\n",
      "Epoch: 18/250  Loss: 0.0043  Val Loss: 0.0099\n",
      "Epoch: 19/250  Loss: 0.0040  Val Loss: 0.0095\n",
      "Epoch: 20/250  Loss: 0.0038  Val Loss: 0.0098\n",
      "Early stopping at epoch: 20\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, train_dataloader, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_str = train_tags_vocab.get_itos()\n",
    "\n",
    "def predict(model, tokens):\n",
    "    padding_size = SEQ_LENGTH - len(tokens) - 2\n",
    "    sent_tensor = torch.Tensor([train_vocab[token] for token in tokens])\n",
    "    token_tensor = torch.unsqueeze(process_sentence(sent_tensor), 0)\n",
    "    token_tensor = token_tensor.to(device, dtype=torch.int32)\n",
    "\n",
    "    model.eval()\n",
    "    output = model(token_tensor)\n",
    "    output = output.detach().cpu().numpy()\n",
    "    output = np.squeeze(output)[1:-padding_size-1]\n",
    "\n",
    "    output_idx = np.argmax(output, axis=1)\n",
    "    tags = []\n",
    "    for idx in output_idx:\n",
    "        tags.append(idx_to_str[idx])\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dev_in_file(filename):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # Split the entire file into sentences. Output: List of sentences\n",
    "    sentences = file_content.strip().split('\\n\\n')\n",
    "\n",
    "    # Split each sentence into their tokens\n",
    "    # Output: List of sentences. Each sentence is a list of tokens\n",
    "    tokens = [i.split('\\n') for i in sentences]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def predict_dev_in(model, filename, output_filename):\n",
    "    sentences = []\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "        sentence = list()\n",
    "        for line in lines:\n",
    "            formatted_line = line.strip()   \n",
    "            \n",
    "            if(len(formatted_line) ==0):\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            sentence.append(formatted_line)\n",
    "\n",
    "    with open(output_filename, \"w\") as wf:\n",
    "        for sentence in sentences:\n",
    "            pred = predict(model, sentence)       \n",
    "            for i in range(len(sentence)):\n",
    "                wf.write(sentence[i] + \" \" + pred[i] + \"\\n\")\n",
    "                \n",
    "            wf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dev_in(model, 'dataset/dev.in', 'dataset/dev.out.lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 3809 tokens with 210 phrases; found: 178 phrases; correct: 77.\n",
      "accuracy:  29.97%; (non-O)\n",
      "accuracy:  93.31%; precision:  43.26%; recall:  36.67%; FB1:  39.69\n",
      "         negative: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "          neutral: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "         positive: precision:  43.26%; recall:  56.20%; FB1:  48.89  178\n",
      "(43.258426966292134, 36.666666666666664, 39.69072164948454)\n"
     ]
    }
   ],
   "source": [
    "from conlleval import evaluate\n",
    "\n",
    "def eval(pred,gold):\n",
    "    f_pred = open(pred,encoding = 'utf-8')\n",
    "    f_gold = open(gold,encoding = 'utf-8')\n",
    "    data_pred = f_pred.readlines()\n",
    "    data_gold = f_gold.readlines()\n",
    "    gold_tags = list()\n",
    "    pred_tags = list()\n",
    "    \n",
    "    for sentence in range(len(data_gold)):\n",
    "        words_pred = data_pred[sentence].strip().split(' ')\n",
    "        words_gold = data_gold[sentence].strip().split(' ')  \n",
    "        if len(words_gold)==1:\n",
    "            continue\n",
    "        # Write original word and predicted tags\n",
    "        gold_tags.append(words_gold[1])\n",
    "        pred_tags.append(words_pred[1])\n",
    "        # End of sentence, write newline\n",
    "    return gold_tags,pred_tags\n",
    "\n",
    "\n",
    "g_tags, p_tags = eval('dataset/dev.out.lstm', 'dataset/dev.out')\n",
    "print(evaluate(g_tags,p_tags,verbose=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ml_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "868b7fb96f33f69c77b53090836525b77aeeeabc65f63ab10b8eda185580798a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
